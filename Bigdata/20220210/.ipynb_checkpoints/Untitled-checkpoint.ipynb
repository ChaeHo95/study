{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c6061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09e3872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3d7bf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "[{'message': 'Rate limit exceeded', 'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9740/3147684922.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#twitter API를 사용하여 단어를 검색하여 트윗들을 크롤링한뒤, 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'패션'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 특정 키워드 크롤링을 위해 search() 함수 사용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_rate_limit_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: [{'message': 'Rate limit exceeded', 'code': 88}]"
     ]
    }
   ],
   "source": [
    "# 발급 완료된 Key를 입력\n",
    "CONSUMER_KEY = 'yYZ7GBu4iQd9SYbpMRTOvqssJ'\n",
    "CONSUMER_SECRET = 'yWXW08kPc35ygi0BnNByUBCGG5TnEoVZ9AsxGS9UVzVKGo4gaW'\n",
    "ACCESS_TOKEN_KEY = '1486551414822236162-BfSelTiGFomedeTry2CUQwHkNLmy7x'\n",
    "ACCES_TOKEN_SECRET = 'JaYHN0dYyJ8B8QwaGiTkpJfXVs1AY1Gvtn48eyttXbbdH'\n",
    "\n",
    "# 개인정보 인증을 요청하는 Handler\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "# 인증 요청을 수행\n",
    "auth.set_access_token(ACCESS_TOKEN_KEY, ACCES_TOKEN_SECRET)\n",
    "\n",
    "# twitter API를 사용하기 위한 준비\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#twitter API를 사용하여 단어를 검색하여 트윗들을 크롤링한뒤, 추출\n",
    "keyword = '패션'\n",
    "tweets = api.search(keyword) # 특정 키워드 크롤링을 위해 search() 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c5028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 메타데이터에 접근 하기 위해 entities() 함수 사용\n",
    "for tweet in tweets:\n",
    "    print(tweet.entities['user_mentions'])\n",
    "    print(tweet.entities['hashtags'])\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링된 데이터를 저장할 데이터 프레임 \n",
    "columns = ['created','tweet_text']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "# 크롤링을 수행할 갯수를 입력하고, Cursor 객체를 사용하여 크롤링을 수행\n",
    "max_tweets = 1000\n",
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=keyword).items(max_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d70cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드가 포함된 1000개의 트윗들에서. 'text','created_at' 정보를 데이터 프레임으로 저장\n",
    "\n",
    "for tweet in searched_tweets:\n",
    "    tweet_json = tweet._json\n",
    "    tweet_text = tweet._json['text']\n",
    "    created = tweet._json['created_at']\n",
    "    row = [created, tweet_text]\n",
    "    series = pd.Series(row, index = df.columns)\n",
    "    df = df.append(series, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거\n",
    "def text_cleaning(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('',text)\n",
    "    return result\n",
    "\n",
    "# tweet_text 피처에 이를 적용\n",
    "df['ko_text'] = df['tweet_text'].apply(lambda x: text_cleaning(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "korea_stopwords_path = 'korean_stopwords.txt'\n",
    "\n",
    "with open(korea_stopwords_path, encoding = 'utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "\n",
    "stopwords = [x.strip() for x in stopwords]\n",
    "\n",
    "def get_nouns(x):\n",
    "    nouns = Okt().nouns(x)\n",
    "    \n",
    "    # 한 글자 키워드를 제거\n",
    "    nouns = [noun for noun in nouns if len(noun) > 1]\n",
    "    \n",
    "    # 불용어를 제거\n",
    "    nouns = [noun for noun in nouns if noun not in stopwords]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "# ko_text 피처에 이를 적용\n",
    "df['nouns'] = df['ko_text'].apply(lambda x:get_nouns(x))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ffaf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# 장바구니 형태의 데이터(트랙젼션 데이터 )를 생성\n",
    "transactions = [\n",
    "    ['패션','블랙'],\n",
    "    ['패션','빈티지'],\n",
    "    ['패션','명품','유행'] \n",
    "]\n",
    "\n",
    "#연관 분석을 수행\n",
    "results = list(apriori(transactions))\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지지도 0.5, 신뢰도 0.6, 형상도 1.0 이상이면서 ('패션','블랙')처럼 규칙의 크기가 2 이하인 규칙을 추출\n",
    "list(apriori(transactions,\n",
    "            min_support=0.5,\n",
    "            min_confidence = 0.6,\n",
    "            min_lift = 1.0,\n",
    "            max_length = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3a3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 트랜잭션 데이터를 추출\n",
    "transactions = df['nouns'].tolist()\n",
    "#공백 문자열을 방지\n",
    "transactions =[transaction for transaction in transactions if transaction]\n",
    "print(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b0bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 연관 분석을 수행\n",
    "results = list(apriori(transactions,\n",
    "            min_support=0.05,\n",
    "            min_confidence = 0.1,\n",
    "            min_lift = 5,\n",
    "            max_length = 2))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임  형태를 정리\n",
    "columns = ['source','target','support']\n",
    "network_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "#규칙의 조건절 ''source', 결과절을 'target',지지도를 'support'라는 데이터 프레임의 피처로 변환\n",
    "for result in results:\n",
    "    if len(result.items) == 2:\n",
    "        items = [x for x in result.items]\n",
    "        row = [items[0], items[1], result.support]\n",
    "        series = pd.Series(row, index = network_df.columns)\n",
    "        network_df = network_df.append(series, ignore_index = True)\n",
    "        \n",
    "network_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치를 추출\n",
    "tweet_corpus = ''.join(df['ko_text'].tolist())\n",
    "\n",
    "#명사 키워드를 추출\n",
    "nouns = Okt().nouns(tweet_corpus)\n",
    "count = Counter(nouns)\n",
    "\n",
    "# 한 글자 키워드를 제거\n",
    "remove_char_counter = Counter({x : count[x] for x in count if len(x) > 1})\n",
    "\n",
    "# 키워드와 키워드 빈도 점수를 'node', 'nodesize'라는 데이터 프레임의 피처로 생성\n",
    "node_df = pd.DataFrame(remove_char_counter.items(), columns = ['node','nodesize'])\n",
    "# 시각화의 편의를 위해 'nodesize' 50 이하는 제거\n",
    "node = node_df[node_df['nodesize']>=50]\n",
    "node.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5d7c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "# networkx 그래프 객체를 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# node의 키워드 빈도수를 데이터로 하여 네트워크 그래프의 '노드' 역활을 하는 원을 생성\n",
    "\n",
    "for index, row in node.iterrows():\n",
    "    G.add_node(row['node'], nodesize = row['nodesize'])\n",
    "\n",
    "# network_df의 연관 분석 데이터를 기반으로 네트워크 그래프의 '관계'  역활을 하는 선을 생성\n",
    "for index, row in network_df.iterrows():\n",
    "    G.add_weighted_edges_from([(row['source'],row['target'], row['support'])])\n",
    "\n",
    "# 그래프 디자인과 관련된 파라미터를 설정\n",
    "pos = nx.spring_layout(G,k=0.6,iterations=50)\n",
    "sizes = [G.nodes[node]['nodesize']*25 for node in G]\n",
    "nx.draw(G, pos = pos, node_size = sizes)\n",
    "\n",
    "#윈도우 사용자는 AppleGothic 대신, 'Malgun Gothic'. 그 외 OS는 OS에서 한글을 지원하는 기본 폰트를 입력\n",
    "nx.draw_networkx_labels(G, pos= pos, font_family = 'Malgun Gothic', font_size = 25)\n",
    "\n",
    "# 그래프 출력\n",
    "ax = plt.gca()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ada66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
